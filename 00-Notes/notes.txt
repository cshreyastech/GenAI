03-LLMs
---------
3.1.2 Lesson Overview
- How LLMs Work
    - LLM Types
    - LLM Intelligence
    - What is a prompt?
    - Open vs Closed Models
    
- How to Use LLMs
    - Inference Settings
    - Conversation History
    - Vicious and Virtuous Cycles
    - Using your intuition

3.1.4: Encoder vs Decoder
Encoder-Decoder Model(T5): Text to Text Transer Transformer
Encoder-Only Model: Uses only 15% of tokens in the dataset
Decoder-Only Model: Uses auto regressor, Uses 100% of the tokens in dataset

3.1.5 Completion vs. Instruction following Models
Decoder: Generative model
Document Completion

3.1.8
Greedy decoding

https://platform.openai.com/
https://chatgpt.com/

3.1.11: What is a Prompt?
P(Y|X, theta)
X:
    - User Prompt
    - Features
Y:
    - LLM Response
    - Label / Prediction
theta:
    - LLM Weights
    - Model Weights


Language fluency conditioned on the user prompt: Providing detail prompt

Auto-regesressive model
- Virtuous Feedback Loop
    - Chain of thoughts prompting: Where the LLM can provide inference time context for answering the user query.
- Vicious Feedback loop
    - Whereas LLM-generated repetions, such as the repeating of a token or phrase, serves as an example of a vicious feedback loop.

3.1.14 Prompt Design Vs. Prompt Engineering
Triggering

3.1.15 Vocareu OpenAI Key
OpenAI Python Package Version 0
import openai
https://platform.openai.com/account/api-keys
openai.api_base = "https://openai.vocareum.com/v1"
openai.api_key = "voc-00000000000000000000000000000000abcd.12345678"
----
OpenAI Python Package Version 1
from openai import OpenAI
client = OpenAI(
    base_url = "https://openai.vocareum.com/v1",
    api_key = "voc-00000000000000000000000000000000abcd.12345678"
)
----
If using curl or another client to access the OpenAI API, build the URLs using
https://openai.vocareum.com/v1 in place of https://api.openai.com/v1

3.2.2 : Lesson Overview
- Intro to NLP: Introdution to different NLP tasks and applications and the challenges in working with language data
- Encoding Text Data: How to encode text data using tokenization and dmbeddings
- Text Generation: Techniques for the NLP task of text generation using recurrent Neural Network

3.2.8: Encoding Text data
- Tokenization breaks text into chunks called "tokens" and encodes these as numerical representation
- Embedding encode context into a vector representation.

3.2.9: Normalization and Pretokenization
Steps for Tokenization
- Normalization: Cleans the text for consistency
- Pretokenization: Break into smaller pieces
- Tokenization
- Postprocessing

3.2.15 NLP Models and Handling Sequences
    - Sequence to single value
    - Sequence to Sequence
    - Encoder-Decoder

3.2.17 Autoregressive Models
Generates one token at at time after the input is given. It takes the input to build context about the input before generating a token, then it takes in the newly generated token as input to add to its context before generating a second new token. This process repeats a number of times until a special of end of sequence token is generated.

Autoregressive models are able to make arbitrarily long sequences by feeding in previously generated tokens to add to the context. The context will hold not just what the latest token was, but also tries to keep track of previous tokens. The model generates the next token by randomly choosing from a set of tokens, each weighted by how likely that model thinks the next token will be that one. The tokens weights are referred to as probabilities, and the process of choosing a token is called sampling.

Token weight -> probabilities
Choosing tokens -> Sampling

3.2.18 Sampling methods for tokens
    - Temperature: 
        - Adjust ramdomness for next token
        - increases or decrease creativity
        - Low Temperature
            - Decreases randomness
                - Most likely tokens chosen more often
            - Verly low temperatures
                - Most likely token always chosen
                - Deterministic models
        - High temperature
            - Increases randomness
                - Low probability tokens now more likely
            - Very high temperatures
        - Chance unlikely token chosen even with low temperatures
    - Top-k Sampling
        - Restrict set of tokens to choose
        - Setting k to 1
            - Most likely token always chosen
    - Adjusting Token sampling
        - Top-k and temperatue will have similar effects
        - Best when used together
            - Temperature: adjusts creativeity of chosen token
            - Top-K: limits how creative oputput can be
    - Nucleus / top-p sampling
    - Beam search
                - More "creative" and "novel" text

03_03 Transformer and Attention Mechanism
------------

03.03.03 Attention and RNNs
- RNNs
    - Vanishing gradient problems
    - Does not take advantage of GPUs as they are serially calculated
    - LSTM
- Embedding from Language Models (ELMo)
    - Bidirectional LSTM
    - Contextualized embeddings
    - Same limitations as LSTM
- Attention solving Sequential dependence

03.03.04 Attention Scores
- Multiplicative Attention
    - Attention(Q, K, V) = softmax(QK^T)V
    - AKA dot product attention, Luong attention
    - Most common in transformers(BERT, GPT)
    - Fast and space-efficient
        - As dimention of Q, K, V are the same 
- Additive Attention
    - Attention(Q, K, V) = softmax(W2tanh(QW1 + (KW3)'))V
    - AKA Bahdahau attention
    - Flexible query, key dimensions
    - Valuable for neural machine tranlation (NMT)
        - Alignment: mapping words in input sequence to workes in output sequence
- General Attention
    - GeneralAttention(Q, K, V) = softmax(QWgK^T )V
    - Flexible query, key dimensions
    - More efficient than additive attention
        - Better speed and space efficieny
    - Less efficient than multiplicative attention

03.03.06 Attention Mechanisms
    - Self-Attention
        - Query = Key = Value
        - O(n)^2
    - Multi-Head Attention
        - Splits attention to multiple heads
            - Embedding dimension divided among heads
        - Each has own set of weights
    - Multi-Query Attention
        - Similar to multi-head, but each query is distinct and shares the same key and value
    - Cross-Attention

03.03.07 Visualilzing Bias in Attention Mechanisms
https://github.com/jessevig/bertviz?tab=readme-ov-file#self-attention-models-bert-gpt-2-etc

03.03.11 Training Transformer-Based Models
- Pre-Training Objectives
    - Autoregressive(AR): predict next token prediction, e.g. GPT
    - Denoising autoencoder: corruption pretext task, e.g BERT
    - Contrastive e.g SentenceBERT, image <--> text CLIP
        - Constructs(positive, negative) pairs
        - Key component of DALL-E for image generation
- GPT: Decoder Transformers

03.03.14 Pros and Cons of Transformers
                        
                          Transformers   RNNs/LSTMs
Training parallelism      Yes            No
Long range memory         Yes            Vanishing gradients
Input type flexibility    Yes            Sequence only
Interpretability          Yes            No
Computation Complexity    Quadratic      Linear
Token length limit        4K - 200K      Way higher

3.4 Retrieval Augmented Generation
----------------------------------
3.4.15 Semantic Text Search and Cosine Similarity
    - Cosine Similarities: Similarities between two vectors
        - Dot product / equlidian magnitude
        - range
            1 : Two vectors are identical
            -1: Diamatically opposite
            0 : Orthogonal to each other. Meaning unrelated
    - Cosine distance: 1 - Cosine Similarity value
        - 0 : Two vectors are identical
        - 1 : Orthogonal to each other. Meaning unrelated
        - 2 : Diamatically opposite


3.4.19 How much context to Provide
Tokens in OpenAI: https://platform.openai.com/tokenizer

3.5 Build Custom Dataset for LLMs
----------------------------------
3.5.2 Lesson Overview
- Open Domain vs. Narrow Domain
    - Zero shot question answering
    - Open domain
        - Wolo[eoda
    - Narrow domain
        - Law, or other specialized data

3.5.3 Collcting data
- Internet data
    - API
        - json, xml
    - Scraping
        - http request
        - costly
        - Legal and ethical consideration
            - Licensing
        - Not safe
            - data "toxicity
            
3.5.9 Evaluating Data Quality
- The text Syntax and Semantics normal for the task (Gramatically correct)
- The text is relavent for the task

3.5.12 Review: Language Modeling Tasks
- Text generation
    - Masked Language modeling
    - Causal language moduling
         - Abstractive summarization
         - Question answering
             - Extrative Quenstion answering
             - Abstractive Quenstion answering
    - Clasification
        - Translation
    - Clustering
- Models and Tasks
    - Autoregressive(decoder-only) - GPT
        - Insttictopm-based text generation
        - Summarization
        - Abstractive question answering
    - Autoencoding (encoder-only) - BERT
        - Classification
        - Extractive question answering
        - Clustering
    - Sequence-to_sequnce(encoder-decoder) -TS
        - Translation


4 Computer Vision and Generative AI
------------------------------------
4.1 Introduction to Image Generation
4.1.5 Compute Vision Gen-AI Models
    - Unconditioned: Unlabled or no label
    - Conditioned Generative Model: Based on prompt
        - Text to Image
        - BLIP: Image to Text
        - Text to Video
        - Multi-modal (text, images, audio)
        - LLaVA: Large Language amd Vision Assistant
 - Generate 2D image
 - Generate 3D asset based on 2D image
 
4.5 Diffusion Moidels

4.5.1 Diffusion Moidels
 - Two phases (Unconditioning: i.e it can generate any image)
     - Forward phase
         - Start with image -> gradually add noise until there is only noise
     - Backward phase
         - Start with noise -> gradully filter it unil there is only image
- Denoising Diffusion Probabilistic Models (DDPM)

4.5.2 Conditioning
    - Conditioning the model the generate something specific
    - Text conditioning
    - Embeddings from Language models
    - Classifier free guidance
    - gamma: Classifier - Free guidance scale

4.5.4 Latent Diffusion Models
    - Autoencoders: embeddings for images also called latent vectors
    - Encoder: Takes the input image and summarizes its content in a vector
    - Image embedding: The inner representation is called image embedding or latent Vector
    - Decoder: Takes the same vector and expands it back to the image
    Latent diffustion Models training (refer to screen shot) is done as size of the latent is less than the size of the image.
    
4.5.4 Diffusion Models: Pros and Cons
    - Pros
        - Generate images with great diversity
        - Great sample quality
        - Easy to contidion
            - text to image
            - image to text
    - Cons
        - Slow

4.5.7 DDPM implementation
    - Noise schedule: Diffusion
4.5.7.b DDPM implementation
    - Temporal Embedding
        - Positional encodings for 4 time steps

05.Building Generative AI Solutions
----------------------------------
05.01 Introcduction to Building Generative Apps
---
05.01.11 Generative AI Solution Components

User Inerface -> Application Logic -> Functional LangChain SDKs / LLMs Image Models -> Database Vecor DB -> APIs External Applications
- User interface: users interact with the application. Its the bridge beween users and AI models.
- Application Logic: Its the controller of the application. It processes user inputs, interacts with AI models, as well as parsing and manageing the flow of data.
- Database: store user information and AI responses or be used alongside OpenAI functions or LangChain tools, or you can use them to fetch data to augment in RAG solutions. This can also be a vector databases in your app for semantic search solutions.
- APIs: Connect our application with external service and platforms.

05.02 Vector Databases
---
05.02.01 Introduction to Vector Databases
    - Long term memory
    - User query + Knowledge base(vector db) -> Context aware prompt -> Model completion -> Answer
    - Vector search
        - Vector distance
            - k-near neighbor search
    - Basic operations of vector databases
        - Storing and managing data
        - Querying vector databases
    - Indexing
        - KNN vs ANN
        - ANN techniques

05.02.04 Vector search
    - Distance between vectors
    - How to use the distance and find similar items
    - Length of the vector -> dimension
    - Vectors in same column has same number of dimensions
    - Query vector should also have the same number of dimensions
    - Euclidean distance
    - KNN
    - Cosine distance (angular distance) 
    cos(theta) = A.B / (|A|*|B|)

05.02.10 Vector search tools
    - Vector index
        - Light-weight libraries for fast vector search
        - FAISS, Annoy
        - Missing data management features of a database
    - Traditional DBs
        - Traditional databases with vector search
        - Pgvector, Elastic, etc
        - Not scallable
        - Unoptimized
    - Vector Databases
        - Unstructed data management solution
        - LanceDB, Milvus, Pinecone, etc.
        - More complete offering
        - Data management
            - Filter
            - Query
            - Indexing
            - Post-processing
            - Updates
            - Versioning
            - Cache <- object stroage
            - Vectors -> indexing
            - Flexibility of queries
                - Metadata
                - Images
                - Text
                - Vector Search
                - Keyword search
                - SQL
https://lancedb.com/docs/quickstart/#how-to-search-for-approximate-nearest-neighbors

05.02.13 Advanced Vector Operations
 - Computing vector distances with thousands of dimensions is expensive
 - Approximate Nearest Neighbors(ANN) gives us a way to trade accuracy for spped
 - There are many ANN methods
     - Hashing
     - Trees
     - Partitions
     - Graphs
     
05.03 Lang Chain
---
05.03.01 LLM Applications
    - Chatbots
    - Cotext generation
    - Language translation
    - Sentiment analysis
    - Text summarization
    - Question-answering system
    - Personalized recomendations
    - Generation of Synthetic Data
Challenges
    - Datasets size and location: Data can be spread across many databases and  or applications
    - Limitations of Request: Popular LLMs only process requests between 250 - 5000 words.
    - Context-Free: LLMs do not retain information from past interactions.
    - OUtput: Output is not structured
LangChain
    - popular framework with many pre-build components
    - Simplifies common use cases
    - Provides abstractions that:
        - Simplify loading data from sources
        - Templatize LLM prompts
        - Parse LLM output
        - Add contextual memory to interactions

05.03.02 Introduction to LLM
    - Completion Models: These models take a string input and provide text that semantically extens the input. They are useful for tasks where the goal is to expand or complete a given text input.
    - Chat-Oriented Models - Designed for conversations, these models can take a dialogue between a user and AI and provide semantic completions or answers. They are particularly suited for interactive applicatoins.

05.03.09 Chains
    - LangChain: Standalone component that allows for the composition of multiple components into a coherent whole. These are referred to as "chains".
    - Chain - A Sequence of calls to various components, which can include other chains. It's a method of stitch together multiple functions and operatons to achieve desired output.

05.03.16 Retrieval Augmented Generation: RAG is a technique that enhances the capabilities of LLM. TAG can integrate a company's data, like a knowledge base, with information contained in the company's own data.

Here is how RAG works
    1) User submit a query to Retrieval Model
    2) Seach with query to knowledge BAse and relevant documents are fetched
    3) Query + Relevant docs are passed to Pre-trained LLM
    4) Response is provided back to the user
    
05.03.17 Vector Stores, Embeddings, and Retrievers
    - Vector Store Index Creator: Integrates vector stores, embeddings, and retrievers, streamlining the process of setting up an information retrieval system.
    - Key steps
        - Document loading: Loading data into the system using various loader components.
        - Document splitting - Documents are split into smaller chunks for more precise matching between user queires and data content.
        - Embeddings Transformation - Each document chunk is transformed intoa embeddings. The embeddings are numerical representation of texzt in a high-dimensional space, where similar meanins are placed close together.
        - Vecgtor Database Storage: Embeddings are stored in a vector database like ChromaDB, disgned for high-dimensionbal vectors and semantic search operatrions.
        - Semantic Search - This system uses ChromaDB to perform semantic searches, finding documents based on contet and meaning.
        - Quesion-Asnwering(QA) Chain - The system uses the output from the semantic search to contextualize and generate response to queries, utilizing semantically relevant documents from the knowledge base.