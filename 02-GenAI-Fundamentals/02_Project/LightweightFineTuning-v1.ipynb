{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f0a19",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db47fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f9bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64fbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bc6957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyas/virtualenvs/genai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# root_dir = \"/tmp\"\n",
    "root_dir = \"./../../../../data/GenAI/02_genai_fundamentals/project2/results\"\n",
    "\n",
    "\n",
    "foundation_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "foundation_model_path = os.path.join(root_dir, \"foundation_model\", \"model\")\n",
    "foundation_model_output_path = os.path.join(root_dir, \"foundation_model\", \"output\")\n",
    "\n",
    "lora_model_path = os.path.join(root_dir, \"lora_model\", \"model\")\n",
    "lora_model_output_path = os.path.join(root_dir, \"lora_model\", \"output\")\n",
    "\n",
    "qlora_model_path = os.path.join(root_dir, \"qlora_model\", \"model\")\n",
    "qlora_model_output_path = os.path.join(root_dir, \"qlora_model\", \"output\")\n",
    "                                \n",
    "                                \n",
    "batch_size = 16\n",
    "train_epochs = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2ab64",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "# dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "#     test_size=0.2, shuffle=True, seed=23\n",
    "# )\n",
    "\n",
    "# splits = [\"train\", \"test\"]\n",
    "\n",
    "# # View the dataset characteristics\n",
    "# dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5077e85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sms_spam\", split=\"train\")\n",
    "dataset = dataset.select(range(3))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=23)\n",
    "\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8846b969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sms': 'Ok lar... Joking wif u oni...\\n', 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first example\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ad30c",
   "metadata": {},
   "source": [
    "## Pre-process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39994a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(foundation_model_name)\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c557c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define tokenization function\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"sms\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",  # or \"longest\" or True\n",
    "#         max_length=128\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7a0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "# tokenized_dataset = {}\n",
    "# for split in splits:\n",
    "#     tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch (this makes tensors!)\n",
    "# for split in splits:\n",
    "#     tokenized_dataset[split].set_format(\n",
    "#         type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbd66c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    foundation_model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in foundation_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(foundation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c8046f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f011867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndValidate(model, model_output_path, model_path):\n",
    "    model.to(device)\n",
    "    \n",
    "    # Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    trainer = Trainer(\n",
    "        model=foundation_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=model_output_path,\n",
    "            # Set the learning rate\n",
    "            learning_rate=2e-5,\n",
    "            # Set the per device train batch size and eval batch size\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            # Evaluate and save the model after each epoch\n",
    "            # evaluation_strategy=\"epoch\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            num_train_epochs=train_epochs,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    trainer.evaluate()\n",
    "    \n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860408d",
   "metadata": {},
   "source": [
    "## Train Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1163f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6523/2507802074.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.742299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.763981</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foundation_trainer = TrainAndValidate(foundation_model, foundation_model_output_path, foundation_model_path)\n",
    "# foundation_model.to(device)\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "# # The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# # Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "# trainer = Trainer(\n",
    "#     model=foundation_model,\n",
    "#     args=TrainingArguments(\n",
    "#         output_dir=foundation_model_output_path,\n",
    "#         # Set the learning rate\n",
    "#         learning_rate=2e-5,\n",
    "#         # Set the per device train batch size and eval batch size\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         # Evaluate and save the model after each epoch\n",
    "#         # evaluation_strategy=\"epoch\",\n",
    "#         eval_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         num_train_epochs=train_epochs,\n",
    "#         weight_decay=0.01,\n",
    "#         load_best_model_at_end=True,\n",
    "#     ),\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00268b",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc84927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foundation_model.to(device)\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6031b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e02a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foundation_model.save_pretrained(foundation_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_CLS\"\n",
    "# )\n",
    "\n",
    "# lora_model = get_peft_model(foundation_model, lora_config)\n",
    "# lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model.to(device)\n",
    "\n",
    "# lora_trainer = Trainer(\n",
    "#     model=lora_model,\n",
    "#     args=TrainingArguments(\n",
    "#         output_dir=\"./tmp/data/lora_spam_not_spam\",\n",
    "#         # Set the learning rate\n",
    "#         learning_rate=2e-5,\n",
    "#         # Set the per device train batch size and eval batch size\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         # Evaluate and save the model after each epoch\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         num_train_epochs=2,\n",
    "#         weight_decay=0.01,\n",
    "#         load_best_model_at_end=True,\n",
    "#     ),\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model.to(device)\n",
    "# lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the model\n",
    "# lora_model.save_pretrained(lora_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f041cd",
   "metadata": {},
   "source": [
    "## Performing Bits and Bytes\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b18a2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,                # 4-bit quantization\n",
    "#     bnb_4bit_use_double_quant=True,   # nested quantization for stability\n",
    "#     bnb_4bit_quant_type=\"nf4\",        # normal float 4 (better accuracy)\n",
    "#     bnb_4bit_compute_dtype=torch.float16,  # compute in half precision\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8fd9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     foundation_model_name,\n",
    "#     num_labels=2,\n",
    "#     quantization_config=bnb_config,\n",
    "#     # device_map=\"auto\",   # automatically place modules on GPU\n",
    "#     id2label={0: \"not spam\", 1: \"spam\"},\n",
    "#     label2id={\"not spam\": 0, \"spam\": 1},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb565e",
   "metadata": {},
   "source": [
    "## Apply LoRA(PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17fd504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlora_model = get_peft_model(qlora_model, lora_config)\n",
    "\n",
    "# qlora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "705c110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlora_model.to(device)\n",
    "\n",
    "# qlora_trainer = Trainer(\n",
    "#     model=qlora_model,\n",
    "#     args=TrainingArguments(\n",
    "#         output_dir=\"./tmp/data/qlora_spam_not_spam\",\n",
    "#         # Set the learning rate\n",
    "#         learning_rate=2e-5,\n",
    "#         # Set the per device train batch size and eval batch size\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         # Evaluate and save the model after each epoch\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         num_train_epochs=2,\n",
    "#         weight_decay=0.01,\n",
    "#         load_best_model_at_end=True,\n",
    "#     ),\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# qlora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2921fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlora_model.to(device)\n",
    "# qlora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71a0778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the model\n",
    "# qlora_model.save_pretrained(qlora_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876ca5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, dataset):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     preds, labels = [], []\n",
    "#     for i in range(0, len(dataset), batch_size):  # batch size = 16\n",
    "#         batch = dataset[i:i+16]\n",
    "#         inputs = {k: torch.tensor(batch[k]) for k in [\"input_ids\", \"attention_mask\"]}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         preds += torch.argmax(outputs.logits, dim=1).tolist()\n",
    "#         labels += batch[\"label\"]\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds)\n",
    "#     return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56267b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = {}\n",
    "# for split in splits:\n",
    "#     tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch (this makes tensors!)\n",
    "# for split in splits:\n",
    "#     tokenized_dataset[split].set_format(\n",
    "#         type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7616ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # Tokenize this batch of text dynamically with padding + truncation\n",
    "        inputs = tokenizer(\n",
    "            batch[\"sms\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        preds += torch.argmax(outputs.logits, dim=1).cpu().tolist()\n",
    "        labels += batch[\"label\"]\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4f4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset=tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_acc, foundation_f1 = evaluate_model(foundation_model, eval_dataset, tokenizer)\n",
    "# lora_acc, lora_f1 = evaluate_model(lora_model, eval_dataset, tokenizer)\n",
    "# qlora_acc, qlora_f1 = evaluate_model(qlora_model, eval_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model - Accuracy: 0.0000, F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Foundation Model - Accuracy: {foundation_acc:.4f}, F1: {foundation_f1:.4f}\")\n",
    "# print(f\"Lora Model - Accuracy: {lora_acc:.4f}, F1: {lora_f1:.4f}\")\n",
    "# print(f\"Qlora Model - Accuracy: {qlora_acc:.4f}, F1: {qlora_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
